import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms


from io import BytesIO
from zipfile import ZipFile
import numpy as np
from PIL import Image
import os
import random
from transforms import *

# Works with https://www.kaggle.com/datasets/soumikrakshit/nyu-depth-v2/data

root = 'nyu_data'

class DepthDataset(Dataset):
    """
    Customized DataSet for grabbing images. DataLoader wraps around this Dataset class to generate batches.

    Args:
        Dataset (torch.utils.Dataset): torch.utils.data.Dataset superclass.
    """
    def __init__(self, data, split, nyu2_split, transform=None):
        """
        Initialize a DepthDataset using output generated by loadZipToMem().

        Args:
            data (dict): A dictionary mapping paths to image bytecode.
            split (str): Either 'train' or 'eval'
            nyu2_train (list of lists): Pairs of input and ground truth paths.
            transform (torchvision.transforms.Transform): Custom transformations that can be applied to images.
        """
        self.split = split
        self.data, self.nyu_dataset = data, nyu2_split
        self.transform = transform

    def __getitem__(self, idx):
        """
        Returns a tuple of the input tensors and ground truth tensors based on index.

        Args:
            idx (integer): The accessed index.

        Returns:
            dict: A dictionary containing "image" and "depth"
        """
        sample = self.nyu_dataset[idx]
        image = Image.open(BytesIO(self.data[os.path.join(root, sample[0])]))
        depth = Image.open(BytesIO(self.data[os.path.join(root, sample[1])]))
        
        # At this point, all depths will fall in between 0-10.0

        sample = {"image": image, "depth": depth}

        if self.transform:
            sample = self.transform(sample)
            
        return sample
    
    def __len__(self):
        """
        Returns the length of the entire dataset.

        Returns:
            integer: Length of DepthDataset. 
        """
        return len(self.nyu_dataset)

class NYU_Testset_Extracted(Dataset):
    def __init__(self, root, resolution='full'):
        self.root = root

        self.files = os.listdir(self.root)


    def __getitem__(self, index):
        image_path = os.path.join(self.root, self.files[index])

        data = np.load(image_path)
        depth, image = data['depth'], data['image']
        depth = np.expand_dims(depth, axis=2)

        image, depth = data['image'], data['depth']
        image = np.array(image)
        depth = np.array(depth)
        return image, depth

    def __len__(self):
        return len(self.files)



class NYU_Testset(Dataset):
    def __init__(self, zip_path):
        input_zip=ZipFile(zip_path)
        data = {name: input_zip.read(name) for name in input_zip.namelist()}
        
        self.rgb = torch.from_numpy(np.load(BytesIO(data['eigen_test_rgb.npy']))).type(torch.float32) #Range [0,1]
        self.depth = torch.from_numpy(np.load(BytesIO(data['eigen_test_depth.npy']))).type(torch.float32) #Range[0, 10]

    def __getitem__(self, idx):
        image = self.rgb[idx]
        depth = self.depth[idx]
        return image, depth

    def __len__(self):
        return len(self.rgb)

def loadZipToMem(zip_file):
    # Load zip file into memory
    print('Loading dataset zip file...', end='')
    input_zip = ZipFile(zip_file)

    data = {name: input_zip.read(name) for name in input_zip.namelist()}

    nyu2_train = list((row.split(',') for row in (data['nyu_data/data/nyu2_train.csv']).decode("utf-8").split('\n') if len(row) > 0))
    nyu2_test = list((row.split(',') for row in (data['nyu_data/data/nyu2_test.csv']).decode("utf-8").split('\n') if len(row) > 0))

    print(f'Loaded (Train Images: {len(nyu2_train)}, Test Images: {len(nyu2_test)}).')
    return data, nyu2_train, nyu2_test

def get_loader(zipfile, batch_size, split):
    ds = get_NYU_dataset(zipfile, split)
    dataloader = DataLoader(ds,
                            batch_size,
                            shuffle=True,
                            num_workers=1
                            )
    return dataloader

def get_NYU_dataset(zipfile, split, uncompressed=True):
    if split == 'train':
        data, nyu2_train, nyu2_test = loadZipToMem(zipfile)

        transform = train_transform()
        dataset = DepthDataset(data, split, nyu2_train, transform=transform)
    elif split == 'eval':
        data, nyu2_train, nyu2_test = loadZipToMem(zipfile)

        transform = eval_transform()
        dataset = DepthDataset(data, split, nyu2_test, transform=transform)
    elif split == 'test':
        if uncompressed:
            dataset = NYU_Testset_Extracted(zipfile)
        else:
            dataset = NYU_Testset(zipfile)

    return dataset

if __name__ == '__main__':
    pass